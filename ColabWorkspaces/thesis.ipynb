{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1yU8CMX1cx3J0Zb6ITdRDHzdn-X1_hO_N",
      "authorship_tag": "ABX9TyPbzf8l2hIeJA/Jqr/hKC4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sammysingle/Thesis/blob/main/ColabWorkspaces/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG-16"
      ],
      "metadata": {
        "id": "HV8gSNxyxejp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "XAyPTfUTxnf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ------------------------------ Build VGG-16 ------------------------------ #\n",
        "# Create base VGG-16 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = VGG16(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of VGG16\n",
        "# Global average pooling\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer 1: 4096 neurons ReLU activation function\n",
        "fc1 = Dense(units = 4096, activation = 'relu')\n",
        "# Fully connected layer 2: 4096 neurons ReLU activation function\n",
        "fc2 = Dense(units = 4096, activation = 'relu')\n",
        "# Classification layer: softmax layer with 8 neurons for class labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of VGG-16\n",
        "model = global_average_pooling(model) # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "model = fc2(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build VGG-16 model\n",
        "vgg16 = Model(inputs, outputs, name = 'VGG-16')\n",
        "\n",
        "# ------------------------------ Train VGG-16 ------------------------------ #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "vgg16.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "vgg16.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save VGG-16 model at end of training\n",
        "save_model(model = vgg16, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ------------------------------ Test VGG-16 ------------------------------ #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "Yw1LnUnixt9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "94OQMZJ8xqx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ------------------------------ Build VGG-16 ------------------------------ #\n",
        "# Create base VGG-16 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = VGG16(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of VGG16\n",
        "# Global average pooling\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer 1: 4096 neurons ReLU activation function\n",
        "fc1 = Dense(units = 4096, activation = 'relu')\n",
        "# Fully connected layer 2: 4096 neurons ReLU activation function\n",
        "fc2 = Dense(units = 4096, activation = 'relu')\n",
        "# Classification layer: softmax layer with 8 neurons for class labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of VGG-16\n",
        "model = global_average_pooling(model) # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "model = fc2(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build VGG-16 model\n",
        "vgg16 = Model(inputs, outputs, name = 'VGG-16')\n",
        "\n",
        "# ------------------------------ Train VGG-16 ------------------------------ #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "vgg16.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "vgg16.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save VGG-16 model at end of training\n",
        "save_model(model = vgg16, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ------------------------------ Test VGG-16 ------------------------------ #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "5rg9zGqBxuXi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "93d1a466-c191-4f74-c2c5-a3bd6d0ab393"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10083 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "   7/2521 [..............................] - ETA: 1:17:28 - loss: 3.6375 - accuracy: 0.1071 - precision_3: 0.1250 - recall_3: 0.0357"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2a85c1760e7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m )\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50"
      ],
      "metadata": {
        "id": "53p5XLdexwen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "RrNMVKJHx4Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ----------------------------- Build ResNet-50 ---------------------------- #\n",
        "# Create base ResNet-50 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = ResNet50(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of ResNet-50\n",
        "global_average_pool = GlobalAveragePooling2D()        # Global average pooling\n",
        "fc1 = Dense(units = 1000, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of ResNet-50\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 1000 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build res_net_50 model\n",
        "res_net_50 = Model(inputs, outputs, name = 'ResNet-50')\n",
        "\n",
        "# ----------------------------- Train ResNet-50 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "res_net_50.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "res_net_50.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save ResNet-50 model at end of training\n",
        "save_model(model = res_net_50, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ----------------------------- Test ResNet-50 ---------------------------- #\n",
        "print('\\n')\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "PNURmEnEx4Zn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee54caa7-7881-4de6-b692-85f2db704237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n",
            "94781440/94765736 [==============================] - 1s 0us/step\n",
            "Found 2433 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "609/609 [==============================] - 691s 1s/step - loss: 2.0874 - accuracy: 0.1944 - precision: 0.3182 - recall: 0.0115 - val_loss: 1.9336 - val_accuracy: 0.1871 - val_precision: 0.4737 - val_recall: 0.0129\n",
            "Epoch 2/200\n",
            "609/609 [==============================] - 34s 55ms/step - loss: 1.9026 - accuracy: 0.2524 - precision: 0.7297 - recall: 0.0333 - val_loss: 1.8475 - val_accuracy: 0.2633 - val_precision: 0.6944 - val_recall: 0.0360\n",
            "Epoch 3/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.8154 - accuracy: 0.2968 - precision: 0.7879 - recall: 0.0748 - val_loss: 1.7992 - val_accuracy: 0.2791 - val_precision: 0.7887 - val_recall: 0.0806\n",
            "Epoch 4/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.7507 - accuracy: 0.3189 - precision: 0.8027 - recall: 0.0986 - val_loss: 1.7591 - val_accuracy: 0.2921 - val_precision: 0.8023 - val_recall: 0.0993\n",
            "Epoch 5/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.7064 - accuracy: 0.3453 - precision: 0.8081 - recall: 0.1143 - val_loss: 1.7303 - val_accuracy: 0.2950 - val_precision: 0.7889 - val_recall: 0.1022\n",
            "Epoch 6/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.6715 - accuracy: 0.3555 - precision: 0.8026 - recall: 0.1254 - val_loss: 1.7029 - val_accuracy: 0.3094 - val_precision: 0.8132 - val_recall: 0.1065\n",
            "Epoch 7/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.6424 - accuracy: 0.3670 - precision: 0.8053 - recall: 0.1377 - val_loss: 1.6607 - val_accuracy: 0.3338 - val_precision: 0.8144 - val_recall: 0.1137\n",
            "Epoch 8/200\n",
            "609/609 [==============================] - 34s 57ms/step - loss: 1.6191 - accuracy: 0.3765 - precision: 0.7852 - recall: 0.1443 - val_loss: 1.6382 - val_accuracy: 0.3525 - val_precision: 0.8163 - val_recall: 0.1151\n",
            "Epoch 9/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.5965 - accuracy: 0.3855 - precision: 0.8088 - recall: 0.1513 - val_loss: 1.6186 - val_accuracy: 0.3597 - val_precision: 0.8056 - val_recall: 0.1252\n",
            "Epoch 10/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.5770 - accuracy: 0.3872 - precision: 0.7971 - recall: 0.1582 - val_loss: 1.5839 - val_accuracy: 0.3928 - val_precision: 0.8073 - val_recall: 0.1266\n",
            "Epoch 11/200\n",
            "609/609 [==============================] - 34s 57ms/step - loss: 1.5594 - accuracy: 0.3995 - precision: 0.7976 - recall: 0.1652 - val_loss: 1.5581 - val_accuracy: 0.4101 - val_precision: 0.7778 - val_recall: 0.1309\n",
            "Epoch 12/200\n",
            "609/609 [==============================] - 39s 64ms/step - loss: 1.5444 - accuracy: 0.4024 - precision: 0.8019 - recall: 0.1714 - val_loss: 1.5494 - val_accuracy: 0.3971 - val_precision: 0.8000 - val_recall: 0.1496\n",
            "Epoch 13/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.5257 - accuracy: 0.4131 - precision: 0.8015 - recall: 0.1776 - val_loss: 1.5310 - val_accuracy: 0.4014 - val_precision: 0.8014 - val_recall: 0.1683\n",
            "Epoch 14/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.5098 - accuracy: 0.4180 - precision: 0.8018 - recall: 0.1862 - val_loss: 1.5075 - val_accuracy: 0.4129 - val_precision: 0.8082 - val_recall: 0.1698\n",
            "Epoch 15/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.4988 - accuracy: 0.4213 - precision: 0.8085 - recall: 0.1961 - val_loss: 1.5033 - val_accuracy: 0.4173 - val_precision: 0.8141 - val_recall: 0.1827\n",
            "Epoch 16/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.4842 - accuracy: 0.4344 - precision: 0.8063 - recall: 0.2002 - val_loss: 1.4881 - val_accuracy: 0.4115 - val_precision: 0.8146 - val_recall: 0.1770\n",
            "Epoch 17/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.4709 - accuracy: 0.4340 - precision: 0.7987 - recall: 0.2072 - val_loss: 1.4891 - val_accuracy: 0.4187 - val_precision: 0.7919 - val_recall: 0.1971\n",
            "Epoch 18/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.4609 - accuracy: 0.4377 - precision: 0.8047 - recall: 0.2100 - val_loss: 1.4762 - val_accuracy: 0.4216 - val_precision: 0.7709 - val_recall: 0.1986\n",
            "Epoch 19/200\n",
            "609/609 [==============================] - 37s 62ms/step - loss: 1.4508 - accuracy: 0.4451 - precision: 0.8080 - recall: 0.2145 - val_loss: 1.4745 - val_accuracy: 0.4201 - val_precision: 0.7647 - val_recall: 0.2058\n",
            "Epoch 20/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.4419 - accuracy: 0.4509 - precision: 0.8069 - recall: 0.2215 - val_loss: 1.4719 - val_accuracy: 0.4230 - val_precision: 0.7536 - val_recall: 0.2245\n",
            "Epoch 21/200\n",
            "609/609 [==============================] - 34s 57ms/step - loss: 1.4323 - accuracy: 0.4542 - precision: 0.7918 - recall: 0.2236 - val_loss: 1.4561 - val_accuracy: 0.4388 - val_precision: 0.7634 - val_recall: 0.2043\n",
            "Epoch 22/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.4245 - accuracy: 0.4546 - precision: 0.8032 - recall: 0.2298 - val_loss: 1.4534 - val_accuracy: 0.4317 - val_precision: 0.7487 - val_recall: 0.2144\n",
            "Epoch 23/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.4153 - accuracy: 0.4603 - precision: 0.8122 - recall: 0.2347 - val_loss: 1.4463 - val_accuracy: 0.4360 - val_precision: 0.7596 - val_recall: 0.2273\n",
            "Epoch 24/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.4069 - accuracy: 0.4616 - precision: 0.8072 - recall: 0.2409 - val_loss: 1.4455 - val_accuracy: 0.4302 - val_precision: 0.7342 - val_recall: 0.2345\n",
            "Epoch 25/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3989 - accuracy: 0.4644 - precision: 0.8068 - recall: 0.2437 - val_loss: 1.4266 - val_accuracy: 0.4388 - val_precision: 0.7488 - val_recall: 0.2317\n",
            "Epoch 26/200\n",
            "609/609 [==============================] - 34s 57ms/step - loss: 1.3911 - accuracy: 0.4694 - precision: 0.8056 - recall: 0.2470 - val_loss: 1.4312 - val_accuracy: 0.4273 - val_precision: 0.7333 - val_recall: 0.2374\n",
            "Epoch 27/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3849 - accuracy: 0.4723 - precision: 0.8077 - recall: 0.2487 - val_loss: 1.4372 - val_accuracy: 0.4417 - val_precision: 0.7387 - val_recall: 0.2360\n",
            "Epoch 28/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3757 - accuracy: 0.4760 - precision: 0.8115 - recall: 0.2548 - val_loss: 1.4490 - val_accuracy: 0.4331 - val_precision: 0.7249 - val_recall: 0.2388\n",
            "Epoch 29/200\n",
            "609/609 [==============================] - 35s 58ms/step - loss: 1.3665 - accuracy: 0.4681 - precision: 0.8139 - recall: 0.2552 - val_loss: 1.4318 - val_accuracy: 0.4518 - val_precision: 0.7455 - val_recall: 0.2360\n",
            "Epoch 30/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3662 - accuracy: 0.4784 - precision: 0.8156 - recall: 0.2581 - val_loss: 1.4383 - val_accuracy: 0.4504 - val_precision: 0.7425 - val_recall: 0.2489\n",
            "Epoch 31/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3571 - accuracy: 0.4747 - precision: 0.8157 - recall: 0.2655 - val_loss: 1.4368 - val_accuracy: 0.4518 - val_precision: 0.7393 - val_recall: 0.2489\n",
            "Epoch 32/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3510 - accuracy: 0.4871 - precision: 0.8185 - recall: 0.2651 - val_loss: 1.4252 - val_accuracy: 0.4532 - val_precision: 0.7325 - val_recall: 0.2403\n",
            "Epoch 33/200\n",
            "609/609 [==============================] - 35s 58ms/step - loss: 1.3418 - accuracy: 0.4887 - precision: 0.8247 - recall: 0.2746 - val_loss: 1.4304 - val_accuracy: 0.4417 - val_precision: 0.7391 - val_recall: 0.2446\n",
            "Epoch 34/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3383 - accuracy: 0.4875 - precision: 0.8242 - recall: 0.2717 - val_loss: 1.4328 - val_accuracy: 0.4489 - val_precision: 0.7306 - val_recall: 0.2576\n",
            "Epoch 35/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.3324 - accuracy: 0.4949 - precision: 0.8232 - recall: 0.2737 - val_loss: 1.4183 - val_accuracy: 0.4604 - val_precision: 0.7457 - val_recall: 0.2489\n",
            "Epoch 36/200\n",
            "609/609 [==============================] - 34s 57ms/step - loss: 1.3273 - accuracy: 0.4949 - precision: 0.8268 - recall: 0.2766 - val_loss: 1.4288 - val_accuracy: 0.4518 - val_precision: 0.7415 - val_recall: 0.2518\n",
            "Epoch 37/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.3178 - accuracy: 0.4973 - precision: 0.8295 - recall: 0.2820 - val_loss: 1.4259 - val_accuracy: 0.4576 - val_precision: 0.7406 - val_recall: 0.2547\n",
            "Epoch 38/200\n",
            "609/609 [==============================] - 50s 82ms/step - loss: 1.3148 - accuracy: 0.5014 - precision: 0.8254 - recall: 0.2836 - val_loss: 1.4314 - val_accuracy: 0.4633 - val_precision: 0.7490 - val_recall: 0.2619\n",
            "Epoch 39/200\n",
            "609/609 [==============================] - 41s 67ms/step - loss: 1.3094 - accuracy: 0.5031 - precision: 0.8310 - recall: 0.2869 - val_loss: 1.4297 - val_accuracy: 0.4619 - val_precision: 0.7377 - val_recall: 0.2590\n",
            "Epoch 40/200\n",
            "609/609 [==============================] - 35s 58ms/step - loss: 1.3006 - accuracy: 0.5055 - precision: 0.8246 - recall: 0.2861 - val_loss: 1.4305 - val_accuracy: 0.4633 - val_precision: 0.7295 - val_recall: 0.2561\n",
            "Epoch 41/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.2964 - accuracy: 0.5072 - precision: 0.8379 - recall: 0.2910 - val_loss: 1.4283 - val_accuracy: 0.4676 - val_precision: 0.7500 - val_recall: 0.2547\n",
            "Epoch 42/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.2885 - accuracy: 0.5109 - precision: 0.8273 - recall: 0.2914 - val_loss: 1.4282 - val_accuracy: 0.4633 - val_precision: 0.7470 - val_recall: 0.2719\n",
            "Epoch 43/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2844 - accuracy: 0.5092 - precision: 0.8308 - recall: 0.2968 - val_loss: 1.4273 - val_accuracy: 0.4691 - val_precision: 0.7551 - val_recall: 0.2662\n",
            "Epoch 44/200\n",
            "609/609 [==============================] - 37s 61ms/step - loss: 1.2784 - accuracy: 0.5191 - precision: 0.8414 - recall: 0.2988 - val_loss: 1.4194 - val_accuracy: 0.4734 - val_precision: 0.7520 - val_recall: 0.2705\n",
            "Epoch 45/200\n",
            "609/609 [==============================] - 41s 68ms/step - loss: 1.2739 - accuracy: 0.5138 - precision: 0.8348 - recall: 0.3033 - val_loss: 1.4186 - val_accuracy: 0.4734 - val_precision: 0.7490 - val_recall: 0.2791\n",
            "Epoch 46/200\n",
            "609/609 [==============================] - 40s 66ms/step - loss: 1.2693 - accuracy: 0.5195 - precision: 0.8369 - recall: 0.3058 - val_loss: 1.4100 - val_accuracy: 0.4705 - val_precision: 0.7481 - val_recall: 0.2777\n",
            "Epoch 47/200\n",
            "609/609 [==============================] - 44s 72ms/step - loss: 1.2665 - accuracy: 0.5121 - precision: 0.8376 - recall: 0.3074 - val_loss: 1.4240 - val_accuracy: 0.4719 - val_precision: 0.7269 - val_recall: 0.2835\n",
            "Epoch 48/200\n",
            "609/609 [==============================] - 43s 71ms/step - loss: 1.2597 - accuracy: 0.5183 - precision: 0.8372 - recall: 0.3087 - val_loss: 1.4098 - val_accuracy: 0.4734 - val_precision: 0.7529 - val_recall: 0.2806\n",
            "Epoch 49/200\n",
            "609/609 [==============================] - 46s 76ms/step - loss: 1.2533 - accuracy: 0.5224 - precision: 0.8420 - recall: 0.3111 - val_loss: 1.4159 - val_accuracy: 0.4748 - val_precision: 0.7568 - val_recall: 0.2820\n",
            "Epoch 50/200\n",
            "609/609 [==============================] - 51s 84ms/step - loss: 1.2479 - accuracy: 0.5273 - precision: 0.8414 - recall: 0.3140 - val_loss: 1.4048 - val_accuracy: 0.4835 - val_precision: 0.7453 - val_recall: 0.2863\n",
            "Epoch 51/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2423 - accuracy: 0.5327 - precision: 0.8360 - recall: 0.3165 - val_loss: 1.4143 - val_accuracy: 0.4878 - val_precision: 0.7424 - val_recall: 0.2820\n",
            "Epoch 52/200\n",
            "609/609 [==============================] - 35s 57ms/step - loss: 1.2392 - accuracy: 0.5335 - precision: 0.8422 - recall: 0.3202 - val_loss: 1.4337 - val_accuracy: 0.4878 - val_precision: 0.7433 - val_recall: 0.2791\n",
            "Epoch 53/200\n",
            "609/609 [==============================] - 34s 55ms/step - loss: 1.2381 - accuracy: 0.5290 - precision: 0.8431 - recall: 0.3181 - val_loss: 1.4203 - val_accuracy: 0.4705 - val_precision: 0.7528 - val_recall: 0.2892\n",
            "Epoch 54/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2367 - accuracy: 0.5331 - precision: 0.8395 - recall: 0.3247 - val_loss: 1.4121 - val_accuracy: 0.4978 - val_precision: 0.7452 - val_recall: 0.2820\n",
            "Epoch 55/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2262 - accuracy: 0.5421 - precision: 0.8349 - recall: 0.3263 - val_loss: 1.4147 - val_accuracy: 0.4950 - val_precision: 0.7500 - val_recall: 0.2935\n",
            "Epoch 56/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2214 - accuracy: 0.5413 - precision: 0.8400 - recall: 0.3300 - val_loss: 1.4174 - val_accuracy: 0.4921 - val_precision: 0.7528 - val_recall: 0.2892\n",
            "Epoch 57/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2186 - accuracy: 0.5351 - precision: 0.8419 - recall: 0.3305 - val_loss: 1.4275 - val_accuracy: 0.4906 - val_precision: 0.7401 - val_recall: 0.2950\n",
            "Epoch 58/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2165 - accuracy: 0.5413 - precision: 0.8414 - recall: 0.3292 - val_loss: 1.4428 - val_accuracy: 0.4849 - val_precision: 0.7393 - val_recall: 0.2978\n",
            "Epoch 59/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2095 - accuracy: 0.5442 - precision: 0.8466 - recall: 0.3313 - val_loss: 1.4195 - val_accuracy: 0.4806 - val_precision: 0.7401 - val_recall: 0.2950\n",
            "Epoch 60/200\n",
            "609/609 [==============================] - 34s 56ms/step - loss: 1.2064 - accuracy: 0.5434 - precision: 0.8405 - recall: 0.3358 - val_loss: 1.4761 - val_accuracy: 0.4777 - val_precision: 0.7266 - val_recall: 0.3022\n",
            "Epoch 61/200\n",
            "607/609 [============================>.] - ETA: 0s - loss: 1.2049 - accuracy: 0.5432 - precision: 0.8454 - recall: 0.3357"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "SfrH0rh1x4fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ----------------------------- Build ResNet-50 ---------------------------- #\n",
        "# Create base ResNet-50 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = ResNet50(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of ResNet-50\n",
        "global_average_pool = GlobalAveragePooling2D()        # Global average pooling\n",
        "fc1 = Dense(units = 1000, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of ResNet-50\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 1000 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build res_net_50 model\n",
        "res_net_50 = Model(inputs, outputs, name = 'ResNet-50')\n",
        "\n",
        "# ----------------------------- Train ResNet-50 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "res_net_50.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "res_net_50.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save ResNet-50 model at end of training\n",
        "save_model(model = res_net_50, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ----------------------------- Test ResNet-50 ---------------------------- #\n",
        "print('\\n')\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "hLXm5f3Gx4lS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "4235ef96-f36d-437c-9448-4f2daaf3e0ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10083 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "   7/2521 [..............................] - ETA: 20:29 - loss: 2.8904 - accuracy: 0.1786 - precision_5: 0.0000e+00 - recall_5: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-90c99410ae8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseNet121"
      ],
      "metadata": {
        "id": "sdG_JzLPyESP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "Hp2r4QICyTgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# --------------------------- Build DenseNet121 ---------------------------- #\n",
        "# Create base DenseNet121 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = DenseNet121(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of DenseNet121\n",
        "# 7x7 global average pooling layer\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer using softmax activation function for 8 labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output classifier\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of DenseNet121\n",
        "model = global_average_pooling(model) # Fully connected layer: 7x7 global average pooling\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build DenseNet121 model\n",
        "dense_net_121 = Model(inputs, outputs, name = 'DenseNet121')\n",
        "\n",
        "# --------------------------- Train DenseNet121 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "dense_net_121.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "dense_net_121.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save DenseNet121 model at end of training\n",
        "save_model(model = dense_net_121, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# --------------------------- Test DenseNet121 ---------------------------- #\n",
        "print('\\n')\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "D5W_qLovyWva",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "6dbed91d-d00e-4cfe-e321-616cfb0ac9dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 1s 0us/step\n",
            "29097984/29084464 [==============================] - 1s 0us/step\n",
            "Found 2433 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "  7/609 [..............................] - ETA: 4:07 - loss: 2.3962 - accuracy: 0.2857 - precision_6: 0.1667 - recall_6: 0.0357"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-490f6bf6a8d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "P7bkI9VO0sx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# --------------------------- Build DenseNet121 ---------------------------- #\n",
        "# Create base DenseNet121 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = DenseNet121(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of DenseNet121\n",
        "# 7x7 global average pooling layer\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer using softmax activation function for 8 labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output classifier\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of DenseNet121\n",
        "model = global_average_pooling(model) # Fully connected layer: 7x7 global average pooling\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build DenseNet121 model\n",
        "dense_net_121 = Model(inputs, outputs, name = 'DenseNet121')\n",
        "\n",
        "# --------------------------- Train DenseNet121 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Training epochs\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "dense_net_121.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "dense_net_121.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save DenseNet121 model at end of training\n",
        "save_model(model = dense_net_121, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# --------------------------- Test DenseNet121 ---------------------------- #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "nT16d9Dv0vOZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "79687c11-53ff-474d-d7e6-dfb51b1c6d31"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10083 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "   7/2521 [..............................] - ETA: 21:21 - loss: 2.9128 - accuracy: 0.1429 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-8dde003c71e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception V3"
      ],
      "metadata": {
        "id": "k7P0fWWJ4-qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "ZcLZ2gZJ5CxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build InceptionV3 --------------------------- #\n",
        "# Create base InceptionV3 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = InceptionV3(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "fc1 = Dense(units = 2048, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 2048 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "inception_v3 = Model(inputs, outputs, name = 'InceptionV3')\n",
        "\n",
        "# ---------------------------- Train InceptionV3 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "inception_v3.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "inception_v3.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save Inception V3 model at end of training\n",
        "save_model(model = inception_v3, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test InceptionV3 -------------------------- #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "KuSDhW3b5FhB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "c7dad0ff-0a9b-4750-c9ed-6e71e90d816c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n",
            "Found 2433 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            " 16/609 [..............................] - ETA: 3:23 - loss: 6.2140 - accuracy: 0.2031 - precision_8: 0.2400 - recall_8: 0.1875"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2605c2ebfdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "A-5dimh75Fwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build InceptionV3 --------------------------- #\n",
        "# Create base InceptionV3 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = InceptionV3(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "fc1 = Dense(units = 2048, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 2048 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "inception_v3 = Model(inputs, outputs, name = 'InceptionV3')\n",
        "\n",
        "# ---------------------------- Train InceptionV3 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "inception_v3.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "inception_v3.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save Inception V3 model at end of training\n",
        "save_model(model = inception_v3, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test InceptionV3 -------------------------- #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "aUdadcsB5Hkd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "98fd2368-dc12-4cd9-dded-0443bd037ba7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10083 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "   7/2521 [..............................] - ETA: 13:48 - loss: 5.4527 - accuracy: 0.2143 - precision_9: 0.2222 - recall_9: 0.1429"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-344add3711d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNet V2"
      ],
      "metadata": {
        "id": "ZIJ6fCbh5INc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "tfe3hDgr5Llh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build MobileNetV2 --------------------------- #\n",
        "# Create base MobileNetV2 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = MobileNetV2(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "classifier = Dense(units = 8, activation = 'softmax') # 1000 neuron fully connected layer with ReLU activation\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "mobile_net_v2 = Model(inputs, outputs, name = 'MobileNetV2')\n",
        "\n",
        "# ---------------------------- Train MobileNetV2 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "mobile_net_v2.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "mobile_net_v2.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save MobileNetV2 model at end of training\n",
        "save_model(model = mobile_net_v2, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test MobileNetV2 -------------------------- #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "lFr7fNHG5WGm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "4f1d550d-a7a8-45b2-907a-fa2a59630007"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "Found 2433 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            " 40/609 [>.............................] - ETA: 51s - loss: 1.9837 - accuracy: 0.3063 - precision_10: 0.4500 - recall_10: 0.1125"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-fd023cc126ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "GrZgwPcG5WMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build MobileNetV2 --------------------------- #\n",
        "# Create base MobileNetV2 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = MobileNetV2(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "classifier = Dense(units = 8, activation = 'softmax') # 1000 neuron fully connected layer with ReLU activation\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "mobile_net_v2 = Model(inputs, outputs, name = 'MobileNetV2')\n",
        "\n",
        "# ---------------------------- Train MobileNetV2 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 200     # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "mobile_net_v2.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy', Precision(), Recall()]\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "mobile_net_v2.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save MobileNetV2 model at end of training\n",
        "save_model(model = mobile_net_v2, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test MobileNetV2 -------------------------- #\n",
        "# Define testing dataset\n",
        "print('\\n')\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy, precision, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "Nwy6Ikt95Yso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "ed960c08-4372-4788-a357-6ef0325a0b16"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10083 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/200\n",
            "  24/2521 [..............................] - ETA: 13:40 - loss: 2.3511 - accuracy: 0.2292 - precision_11: 0.2222 - recall_11: 0.0417"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-599771712f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "1DLcJb4pZeJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import directory navigation\n",
        "from os import listdir\n",
        "# Import tensorflow libraries for loading models and testing dataset\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "#from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "# Constants for model file paths\n",
        "base_model_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/'\n",
        "models = ['VGG-16', 'ResNet-50', 'DenseNet121', 'InceptionV3', 'MobileNetV2']\n",
        "\n",
        "# ------------------------- Dataset and Testing Functions ------------------------ #\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "def test_model(test_ds, model_path):\n",
        "    # Evaluate model on test dataset for accuracy, precision and recall\n",
        "    model = load_model(model_path)\n",
        "    #loss, accuracy, precision, recall = model.evaluate(test_ds)\n",
        "    print('Accuracy: ' + accuracy)\n",
        "    print('Precision: ' + precision)\n",
        "    print('Recall: ' + recall)\n",
        "    # Evaluate model for confusion matrix #########\n",
        "    #predictions = model.predict(test_ds)\n",
        "    #cm = confusion_matrix(test_ds, np.rint(predictions), labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash'])\n",
        "    #display = ConfusionMatrixDisplay(cm)\n",
        "    #display.plot()\n",
        "    \n",
        "\n",
        "# ----------------------------- Load Testing Dataset ----------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (224, 224),\n",
        "    batch_size = 346,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "test_ds = test_ds.map(process)\n",
        "\n",
        "# ----------------------------- Load and Test Models ----------------------------- #\n",
        "for model in models:\n",
        "    print('\\n# ---------------------------------------- '+model+' ----------------------------------------\\n')\n",
        "    # Original model fully trained\n",
        "    print('Original Dataset Training - Full Training:')\n",
        "    test_model(test_ds, base_model_path+model+'/Original Training/End')\n",
        "    print('\\n')\n",
        "    # Original model callback to best validation performance\n",
        "    print('Original Dataset Training - Callback to Best Validation Performance:')\n",
        "    test_model(test_ds, base_model_path+model+'/Original Training/Checkpoint')\n",
        "    print('\\n')\n",
        "    # Augmented training dataset model fully trained\n",
        "    print('Augmented Dataset Training - Full Training:')\n",
        "    test_model(test_ds, base_model_path+model+'/Augmented Training/End')\n",
        "    print('\\n')\n",
        "    # Augmented training dataset model callback to best validation performance\n",
        "    print('Augmented Dataset Training - Callback to Best Validation Performance:')\n",
        "    test_model(test_ds, base_model_path+model+'/Augmented Training/Checkpoint')"
      ],
      "metadata": {
        "id": "3BcNZ88uZgSc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "fe3f8da1-8d87-4167-ffc3-6663c17f236f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 346 files belonging to 8 classes.\n",
            "\n",
            "# ---------------------------------------- VGG-16 ----------------------------------------\n",
            "\n",
            "Original Dataset Training - Full Training:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-b8dfe5701ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Original model fully trained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original Dataset Training - Full Training:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/Original Training/End'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Original model callback to best validation performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b8dfe5701ea6>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(test_ds, model_path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Evaluate model on test dataset for accuracy, precision and recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m#loss, accuracy, precision, recall = model.evaluate(test_ds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mnodes_to_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeras_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   loaded = tf.__internal__.saved_model.load_partial(\n\u001b[0;32m--> 142\u001b[0;31m       path, nodes_to_load, options=options)\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;31m# Finalize the loaded layers and remove the extra tracked dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_partial\u001b[0;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0mnode\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0mto\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m   \"\"\"\n\u001b[0;32m--> 842\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mload_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    973\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m         loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,\n\u001b[0;32m--> 975\u001b[0;31m                             ckpt_options, options, filters)\n\u001b[0m\u001b[1;32m    976\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, object_graph_proto, saved_model_proto, export_dir, ckpt_options, save_options, filters)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msave_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_skip_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCapturableResource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    558\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m       \u001b[0mload_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m       \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_existing_objects_matched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path, options)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       \u001b[0mdtype_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable_to_dtype_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m       \u001b[0mobject_graph_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOBJECT_GRAPH_PROTO_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m       \u001b[0;31m# The object graph proto does not exist in this checkpoint. Try the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/py_checkpoint_reader.py\u001b[0m in \u001b[0;36mget_tensor\u001b[0;34m(self, tensor_str)\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     return CheckpointReader.CheckpointReader_GetTensor(\n\u001b[0;32m---> 67\u001b[0;31m         self, compat.as_bytes(tensor_str))\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;31m# TODO(b/143319754): Remove the RuntimeError casting logic once we resolve the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m   \u001b[0;31m# issue with throwing python exceptions from C++.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation and Preprocessing"
      ],
      "metadata": {
        "id": "Z17a00QembaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Augmentor"
      ],
      "metadata": {
        "id": "d2COUrNmspTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dataset Split and Preprocessing: 70:20:10 (training, validation, testing) split*"
      ],
      "metadata": {
        "id": "G9yVAClFUxpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import directory navigation libraries and image resizing\n",
        "from os import listdir\n",
        "from shutil import move\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "from tensorflow.image import resize\n",
        "from tensorflow.io import decode_jpeg, encode_jpeg, write_file\n",
        "from tensorflow.dtypes import saturate_cast\n",
        "\n",
        "# Constant values\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/'\n",
        "base_validation_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset/'\n",
        "base_test_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset/'\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# ------------------------------- Functions -------------------------------- #\n",
        "# Resize images\n",
        "def resize_image(path, img_width, img_height):\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        image = open(path+'/'+file, 'rb') # Open image as binary file\n",
        "        binary_representation = image.read() # Read binary file\n",
        "        decoded_representation = decode_jpeg(binary_representation) # Decode image\n",
        "        resized_image = resize(decoded_representation, [img_width, img_height])\n",
        "        resized_image = saturate_cast(resized_image, 'uint8')\n",
        "        encoded_image = encode_jpeg(resized_image)\n",
        "        write_file(path+'/'+file, encoded_image)\n",
        "\n",
        "# Check images for RGB mode and convert if required\n",
        "def convert_rgb_images(path):\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        img = Image.open(path+'/'+file)\n",
        "        if img.mode != 'RGB':\n",
        "            img.convert('RGB').save(path+'/'+file)\n",
        "        \n",
        "# Count number of files in directory\n",
        "def count_files(inputPath, label):\n",
        "    directory = listdir(inputPath+label)\n",
        "    num_files = 0\n",
        "    for file in directory:\n",
        "        num_files += 1\n",
        "    return num_files\n",
        "\n",
        "# Split training and testing dataset\n",
        "def split_dataset(input_path, train_path, val_path, test_path, val_prop, test_prop, label):\n",
        "    # Count images in directory and get files names\n",
        "    num_files = count_files(input_path, label)\n",
        "    file_names = listdir(input_path+label)\n",
        "\n",
        "    # Calculated number of validation and testing images\n",
        "    num_val = round(num_files*val_prop)\n",
        "    num_test = round(num_files*test_prop)\n",
        "    \n",
        "    # Initialise empty lists for validation and testing image indexes\n",
        "    val_list = []\n",
        "    test_list = []\n",
        "    \n",
        "    # Randomly assign file indexes for validation dataset\n",
        "    i = 0\n",
        "    while i < num_val:\n",
        "        r = randint(1, num_files - 1)\n",
        "        if r not in val_list:\n",
        "            val_list.append(r)\n",
        "            i += 1\n",
        "    # Move validation dataset files and rename\n",
        "    name_incrementer = 1\n",
        "    for j in val_list:\n",
        "        move(input_path+label+'/'+file_names[j], val_path+label+'/'+label+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "\n",
        "    # Update number of files and directory for remaining files\n",
        "    num_files = count_files(input_path, label)\n",
        "    remaining_files = listdir(input_path+label)\n",
        "    # Randomly assign file indexes for testing dataset\n",
        "    i = 0\n",
        "    while i < num_test:\n",
        "        r = randint(1, num_files - 1)\n",
        "        if r not in test_list:\n",
        "            test_list.append(r)\n",
        "            i += 1\n",
        "    # Move test dataset files and rename\n",
        "    name_incrementer = 1\n",
        "    for j in test_list:\n",
        "        move(input_path+label+'/'+remaining_files[j], test_path+label+'/'+label+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "\n",
        "    # Move remaining files into training dataset\n",
        "    training_files = listdir(input_path+label)\n",
        "    name_incrementer = 1\n",
        "    for file in training_files:\n",
        "        move(input_path+label+'/'+file, train_path+label+'/'+label+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "\n",
        "# Print number of files in label directories\n",
        "def print_num_files(path, label):\n",
        "    directory = listdir(path+label)\n",
        "    counter = 0\n",
        "    for file in directory:\n",
        "        counter += 1\n",
        "    print(\"Number of files in \" + label +':' + str(counter))\n",
        "\n",
        "# ----------------- Split Training and Validation Datasets ----------------- #\n",
        "# Print split label contents\n",
        "print('# --------------- Total Label Counts ---------------- #')\n",
        "# Split labels base off 70:20:10 training split\n",
        "for label in labels:\n",
        "    print_num_files(base_path+'Unsplit Dataset/', label)\n",
        "    split_dataset(base_path+'Unsplit Dataset/', base_original_training_path, base_validation_path, base_test_path, 0.2, 0.1, label)\n",
        "\n",
        "# Print validation counts for each label\n",
        "print('\\n# ---------- Total Validation Label Counts ---------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_validation_path, label)\n",
        "\n",
        "# Print testing counts for each label\n",
        "print('\\n# ------------ Total Testing Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_test_path, label)\n",
        "\n",
        "# Print training counts for each label in training dataset\n",
        "print('\\n# ----------- Total Training Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)\n",
        "\n",
        "# ------------------------- Pre-Process Images -------------------------- #\n",
        "# Pre-process training dataset\n",
        "print('\\n# --------- Pre-Processing Training Dataset --------- #')\n",
        "for label in labels:\n",
        "    convert_rgb_images(base_original_training_path+label)\n",
        "    resize_image(base_original_training_path+label, 224, 224)\n",
        "# Pre-process validation dataset\n",
        "print('\\n# -------- Pre-Processing Validation Dataset -------- #')\n",
        "for label in labels:\n",
        "    convert_rgb_images(base_original_training_path+label)\n",
        "    resize_image(base_validation_path+label, 224, 224)\n",
        "# Pre-process test dataset\n",
        "print('\\n# --------- Pre-Processing Testing Dataset --------- #')\n",
        "for label in labels:\n",
        "    convert_rgb_images(base_original_training_path+label)\n",
        "    resize_image(base_test_path+label, 224, 224)"
      ],
      "metadata": {
        "id": "_jR-D3y89F57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augementation Set 1: Distort and Flip*"
      ],
      "metadata": {
        "id": "GgyR_bKCOuCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "from os import listdir\n",
        "from time import sleep\n",
        "\n",
        "# Define base path of training dataset\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "# Define labels\n",
        "#labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "labels = ['Glass']\n",
        "\n",
        "# Verify augmentation counts and print\n",
        "def count_augmented_and_print(path):\n",
        "    count_augmented = 0\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        count_augmented += 1\n",
        "    print('Processed with '+str(count_augmented)+' image(s) found.\\n')\n",
        "\n",
        "# Geometric augmentation function: distort and flip\n",
        "def distort_and_flip(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.flip_left_right(1)                   # Horizontally flip every image\n",
        "    pipe.random_distortion(1, 5, 5, 4)        # Randomly distort flipped images\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "    count_augmented_and_print(path+'/output') # Print augmented image count to console\n",
        "    sleep(60)                                 # Sleep program while images processing\n",
        "\n",
        "# Perform augmentation\n",
        "for label in labels:\n",
        "    distort_and_flip(base_original_training_path+label)"
      ],
      "metadata": {
        "id": "WR_iz_J4OtDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmentation Set 2: Rotate and Shear*"
      ],
      "metadata": {
        "id": "9l2NeImgQjJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "from os import listdir\n",
        "from time import sleep\n",
        "\n",
        "# Define base path of training dataset\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "# Define labels\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# Verify augmentation counts and print\n",
        "def count_augmented_and_print(path):\n",
        "    count_augmented = 0\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        count_augmented += 1\n",
        "    print('Processed with '+str(count_augmented)+' image(s) found.\\n')\n",
        "\n",
        "# Geometric augmentation: rotate and shear\n",
        "def rotate_and_shear(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.rotate(1, 25, 25)                    # Rotate every image by 45 degrees\n",
        "    pipe.shear(1, 15, 15)                     # Randomly shear between 0 and 15 degrees\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "    count_augmented_and_print(path+'/output') # Print augmented image count to console\n",
        "    sleep(60)                                 # Sleep program while images processing\n",
        "\n",
        "# Augment images\n",
        "for label in labels:\n",
        "    rotate_and_shear(base_original_training_path+label)"
      ],
      "metadata": {
        "id": "wj0yjMiiQjQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmentation Set 3: Random Erasing*"
      ],
      "metadata": {
        "id": "HiBgc25eQjhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "from os import listdir\n",
        "from time import sleep\n",
        "\n",
        "# Define base path of training dataset\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "# Define labels\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# Verify augmentation counts and print\n",
        "def count_augmented_and_print(path):\n",
        "    count_augmented = 0\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        count_augmented += 1\n",
        "    print('Processed with '+str(count_augmented)+' image(s) found.\\n')\n",
        "\n",
        "# Random occlusion: random erasing \n",
        "def random_erase(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.random_erasing(1, 0.15)              # Randomly erase area in every image\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "    count_augmented_and_print(path+'/output') # Print augmented image count to console\n",
        "    sleep(60)                                 # Sleep program while images processing\n",
        "\n",
        "# Augment images\n",
        "for label in labels:\n",
        "    random_erase(base_original_training_path+label)\n",
        "\n",
        "# Second augmentation\n",
        "for label in labels:\n",
        "    random_erase(base_original_training_path+label+'/output')"
      ],
      "metadata": {
        "id": "AvgK26ktQjoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Rename Files*"
      ],
      "metadata": {
        "id": "lVUDecD6ukHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from os import listdir\n",
        "from shutil import move\n",
        "\n",
        "# Constants\n",
        "set_one_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/1 Distort and Flip/'\n",
        "set_two_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/2 Rotate and Shear/'\n",
        "set_three_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/3 Random Erasing/'\n",
        "set_one_augmentation = 'distort_and_flip'\n",
        "set_two_augmentation = 'shear_and_rotate'\n",
        "set_three_augmentation = 'random_erase'\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# Get files in directory and rename\n",
        "def rename_files(path, base_name, augmentation):\n",
        "    name_incrementer = 1\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        move(path+'/'+file, path+'/'+base_name+'_'+augmentation+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "        print(file)\n",
        "    return name_incrementer\n",
        "\n",
        "# Rename files\n",
        "for label in labels:\n",
        "    rename_files(set_one_path+label+'/output', label, set_one_augmentation)\n",
        "    rename_files(set_two_path+label+'/output', label, set_two_augmentation)\n",
        "    rename_files(set_three_path+label+'/output', label, set_three_augmentation)"
      ],
      "metadata": {
        "id": "3M7P4vonum1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*File Counts*"
      ],
      "metadata": {
        "id": "yhoBeSzIB3HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import file navigation library\n",
        "from os import listdir\n",
        "\n",
        "# Directory paths\n",
        "base_validation_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset/'\n",
        "base_test_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset/'\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "base_augmented_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset/'\n",
        "\n",
        "# Print number of files in label directories\n",
        "def print_num_files(path, label):\n",
        "    directory = listdir(path+label)\n",
        "    counter = 0\n",
        "    for file in directory:\n",
        "        counter += 1\n",
        "    print(\"Number of files in \" + label +':' + str(counter))\n",
        "\n",
        "# Print training counts for each label in training dataset\n",
        "print('\\n# ----------- Total Training Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)\n",
        "\n",
        "# Print validation counts for each label\n",
        "print('\\n# ---------- Total Validation Label Counts ---------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_validation_path, label)\n",
        "\n",
        "# Print testing counts for each label\n",
        "print('\\n# ------------ Total Testing Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_test_path, label)\n",
        "\n",
        "# Print training counts for each label in training dataset\n",
        "print('\\n# ----------- Total Training Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)\n",
        "\n",
        "# Print augmented counts for each label in augmented training dataset\n",
        "print('\\n# ----------- Total Augmented Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)"
      ],
      "metadata": {
        "id": "4iqFvvDOB2dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "\n",
        "# Define path of sample image for each augmentation\n",
        "base_path_set_one = '/content/drive/MyDrive/Colab Notebooks/Data Augmentation Samples/1 Distort and Flip'\n",
        "base_path_set_two = '/content/drive/MyDrive/Colab Notebooks/Data Augmentation Samples/2 Rotate and Shear'\n",
        "base_path_set_three = '/content/drive/MyDrive/Colab Notebooks/Data Augmentation Samples/3 Random Erasing'\n",
        "\n",
        "# Geometric augmentation function: distort and flip\n",
        "def distort_and_flip(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.flip_left_right(1)                   # Horizontally flip every image\n",
        "    pipe.random_distortion(1, 5, 5, 4)        # Randomly distort flipped images\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "\n",
        "# Geometric augmentation: rotate and shear\n",
        "def rotate_and_shear(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.rotate(1, 25, 25)                    # Rotate every image by 45 degrees\n",
        "    pipe.shear(1, 15, 15)                     # Randomly shear between 0 and 15 degrees\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "\n",
        "# Random occlusion: random erasing \n",
        "def random_erase(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.random_erasing(1, 0.15)              # Randomly erase area in every image\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "\n",
        "# Perform augmentations\n",
        "# Distort and flip\n",
        "distort_and_flip(base_path_set_one)\n",
        "# Rotate and shear\n",
        "rotate_and_shear(base_path_set_two)\n",
        "# Random erase\n",
        "random_erase(base_path_set_three)\n",
        "random_erase(base_path_set_three+'/output')"
      ],
      "metadata": {
        "id": "Ml6Fg1d_eI1_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}