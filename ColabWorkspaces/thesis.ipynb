{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1yU8CMX1cx3J0Zb6ITdRDHzdn-X1_hO_N",
      "authorship_tag": "ABX9TyMTaAYGhf2udP6Syha0AytZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sammysingle/Thesis/blob/main/ColabWorkspaces/thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VGG-16"
      ],
      "metadata": {
        "id": "HV8gSNxyxejp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "XAyPTfUTxnf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ------------------------------ Build VGG-16 ------------------------------ #\n",
        "# Create base VGG-16 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = VGG16(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of VGG16\n",
        "# Global average pooling\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer 1: 4096 neurons ReLU activation function\n",
        "fc1 = Dense(units = 4096, activation = 'relu')\n",
        "# Fully connected layer 2: 4096 neurons ReLU activation function\n",
        "fc2 = Dense(units = 4096, activation = 'relu')\n",
        "# Classification layer: softmax layer with 8 neurons for class labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of VGG-16\n",
        "model = global_average_pooling(model) # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "model = fc2(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build VGG-16 model\n",
        "vgg16 = Model(inputs, outputs, name = 'VGG-16')\n",
        "\n",
        "# ------------------------------ Train VGG-16 ------------------------------ #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "vgg16.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "vgg16.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save VGG-16 model at end of training\n",
        "save_model(model = vgg16, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ------------------------------ Test VGG-16 ------------------------------ #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = vgg16.evaluate(test_ds)\n",
        "loss, precision = vgg16.evaluate(test_ds)\n",
        "loss, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "Yw1LnUnixt9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "94OQMZJ8xqx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ------------------------------ Build VGG-16 ------------------------------ #\n",
        "# Create base VGG-16 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = VGG16(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of VGG16\n",
        "# Fully connected layer 1: 4096 neurons ReLU activation function\n",
        "fc1 = Dense(units = 4096, activation = 'relu')\n",
        "# Fully connected layer 2: 4096 neurons ReLU activation function\n",
        "fc2 = Dense(units = 4096, activation = 'relu')\n",
        "# Classification layer: softmax layer with 8 neurons for class labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of VGG-16\n",
        "model = fc1(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "model = fc2(model)                    # Fully connected layer: 4096 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build VGG-16 model\n",
        "vgg16 = Model(inputs, outputs, name = 'VGG-16')\n",
        "\n",
        "# ------------------------------ Train VGG-16 ------------------------------ #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "vgg16.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "vgg16.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save VGG-16 model at end of training\n",
        "save_model(model = vgg16, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/VGG-16/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ------------------------------ Test VGG-16 ------------------------------ #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = vgg16.evaluate(test_ds)\n",
        "loss, precision = vgg16.evaluate(test_ds)\n",
        "loss, recall = vgg16.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "5rg9zGqBxuXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet-50"
      ],
      "metadata": {
        "id": "53p5XLdexwen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "RrNMVKJHx4Qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ----------------------------- Build ResNet-50 ---------------------------- #\n",
        "# Create base ResNet-50 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = ResNet50(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of ResNet-50\n",
        "global_average_pool = GlobalAveragePooling2D()        # Global average pooling\n",
        "fc1 = Dense(units = 1000, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of ResNet-50\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 1000 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build res_net_50 model\n",
        "res_net_50 = Model(inputs, outputs, name = 'ResNet-50')\n",
        "\n",
        "# ----------------------------- Train ResNet-50 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "res_net_50.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "res_net_50.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save ResNet-50 model at end of training\n",
        "save_model(model = res_net_50, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ----------------------------- Test ResNet-50 ---------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = res_net_50.evaluate(test_ds)\n",
        "loss, precision = res_net_50.evaluate(test_ds)\n",
        "loss, recall = res_net_50.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "PNURmEnEx4Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "SfrH0rh1x4fK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ----------------------------- Build ResNet-50 ---------------------------- #\n",
        "# Create base ResNet-50 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = ResNet50(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of ResNet-50\n",
        "global_average_pool = GlobalAveragePooling2D()        # Global average pooling\n",
        "fc1 = Dense(units = 1000, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of ResNet-50\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 1000 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build res_net_50 model\n",
        "res_net_50 = Model(inputs, outputs, name = 'ResNet-50')\n",
        "\n",
        "# ----------------------------- Train ResNet-50 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "res_net_50.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "res_net_50.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save ResNet-50 model at end of training\n",
        "save_model(model = res_net_50, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/ResNet-50/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ----------------------------- Test ResNet-50 ---------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = res_net_50.evaluate(test_ds)\n",
        "loss, precision = res_net_50.evaluate(test_ds)\n",
        "loss, recall = res_net_50.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "hLXm5f3Gx4lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DenseNet121"
      ],
      "metadata": {
        "id": "sdG_JzLPyESP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "Hp2r4QICyTgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# --------------------------- Build DenseNet121 ---------------------------- #\n",
        "# Create base DenseNet121 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = DenseNet121(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of DenseNet121\n",
        "# 7x7 global average pooling layer\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer using softmax activation function for 8 labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output classifier\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of DenseNet121\n",
        "model = global_average_pooling(model) # Fully connected layer: 7x7 global average pooling\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build DenseNet121 model\n",
        "dense_net_121 = Model(inputs, outputs, name = 'DenseNet121')\n",
        "\n",
        "# --------------------------- Train DenseNet121 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "dense_net_121.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "dense_net_121.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save DenseNet121 model at end of training\n",
        "save_model(model = dense_net_121, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# --------------------------- Test DenseNet121 ---------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = dense_net_121.evaluate(test_ds)\n",
        "loss, precision = dense_net_121.evaluate(test_ds)\n",
        "loss, recall = dense_net_121.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "D5W_qLovyWva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "P7bkI9VO0sx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# --------------------------- Build DenseNet121 ---------------------------- #\n",
        "# Create base DenseNet121 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = DenseNet121(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of DenseNet121\n",
        "# 7x7 global average pooling layer\n",
        "global_average_pooling = GlobalAveragePooling2D()\n",
        "# Fully connected layer using softmax activation function for 8 labels\n",
        "classifier = Dense(units = 8, activation = 'softmax')\n",
        "\n",
        "# Connect input layer, base model and output classifier\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of DenseNet121\n",
        "model = global_average_pooling(model) # Fully connected layer: 7x7 global average pooling\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build DenseNet121 model\n",
        "dense_net_121 = Model(inputs, outputs, name = 'DenseNet121')\n",
        "\n",
        "# --------------------------- Train DenseNet121 ---------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Training epochs\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "dense_net_121.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "dense_net_121.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save DenseNet121 model at end of training\n",
        "save_model(model = dense_net_121, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/DenseNet121/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# --------------------------- Test DenseNet121 ---------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = dense_net_121.evaluate(test_ds)\n",
        "loss, precision = dense_net_121.evaluate(test_ds)\n",
        "loss, recall = dense_net_121.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "nT16d9Dv0vOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inception V3"
      ],
      "metadata": {
        "id": "k7P0fWWJ4-qj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "ZcLZ2gZJ5CxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build InceptionV3 --------------------------- #\n",
        "# Create base InceptionV3 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = InceptionV3(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "fc1 = Dense(units = 2048, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 2048 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "inception_v3 = Model(inputs, outputs, name = 'InceptionV3')\n",
        "\n",
        "# ---------------------------- Train InceptionV3 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "inception_v3.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "inception_v3.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save Inception V3 model at end of training\n",
        "save_model(model = inception_v3, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test InceptionV3 -------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = inception_v3.evaluate(test_ds)\n",
        "loss, precision = inception_v3.evaluate(test_ds)\n",
        "loss, recall = inception_v3.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "KuSDhW3b5FhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "A-5dimh75Fwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build InceptionV3 --------------------------- #\n",
        "# Create base InceptionV3 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = InceptionV3(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "fc1 = Dense(units = 2048, activation = 'relu')        # 1000 neuron fully connected layer with ReLU activation\n",
        "classifier = Dense(units = 8, activation = 'softmax') # Softmax classifier\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "model = fc1(model)                    # Fully connected layer: 2048 neuron ReLU\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "inception_v3 = Model(inputs, outputs, name = 'InceptionV3')\n",
        "\n",
        "# ---------------------------- Train InceptionV3 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "inception_v3.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "inception_v3.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save Inception V3 model at end of training\n",
        "save_model(model = inception_v3, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/InceptionV3/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test InceptionV3 -------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = inception_v3.evaluate(test_ds)\n",
        "loss, precision = inception_v3.evaluate(test_ds)\n",
        "loss, recall = inception_v3.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "aUdadcsB5Hkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNet V2"
      ],
      "metadata": {
        "id": "ZIJ6fCbh5INc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Original Dataset*"
      ],
      "metadata": {
        "id": "tfe3hDgr5Llh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build MobileNetV2 --------------------------- #\n",
        "# Create base MobileNetV2 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = MobileNetV2(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "classifier = Dense(units = 8, activation = 'softmax') # 1000 neuron fully connected layer with ReLU activation\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "mobile_net_v2 = Model(inputs, outputs, name = 'MobileNetV2')\n",
        "\n",
        "# ---------------------------- Train MobileNetV2 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "mobile_net_v2.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Original Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "mobile_net_v2.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save MobileNetV2 model at end of training\n",
        "save_model(model = mobile_net_v2, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Original Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test MobileNetV2 -------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = mobile_net_v2.evaluate(test_ds)\n",
        "loss, precision = mobile_net_v2.evaluate(test_ds)\n",
        "loss, recall = mobile_net_v2.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "lFr7fNHG5WGm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c0b690-136d-4766-b933-c00a4cee7cea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "9420800/9406464 [==============================] - 0s 0us/step\n",
            "Found 2433 files belonging to 8 classes.\n",
            "Found 695 files belonging to 8 classes.\n",
            "Epoch 1/50\n",
            "245/609 [===========>..................] - ETA: 1:48 - loss: 1.0918 - accuracy: 0.6184"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmented Dataset*"
      ],
      "metadata": {
        "id": "GrZgwPcG5WMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import os library to navigate directories\n",
        "import os\n",
        "# Import tensorflow functions for model building\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalAveragePooling2D\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import save_model\n",
        "# Import tensorflow functions for training\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.data import AUTOTUNE as autotune\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow import cast\n",
        "# Import tensorflow functions for metrics\n",
        "from tensorflow.keras.metrics import Accuracy, Precision, Recall\n",
        "\n",
        "# ---------------------------- Build MobileNetV2 --------------------------- #\n",
        "# Create base MobileNetV2 model:\n",
        "  # 224x224x3 input size\n",
        "  # Only feature extraction layers - no classification layers\n",
        "  # Feature extraction layer weights trained on ImageNet dataset\n",
        "base_model = MobileNetV2(input_shape = (224, 224, 3),\n",
        "                         include_top = False,\n",
        "                         weights = 'imagenet'\n",
        "                         )\n",
        "# Freeze learning on feature extraction layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create classification layers of InceptionV3\n",
        "global_average_pool = GlobalAveragePooling2D()        # global average pooling\n",
        "classifier = Dense(units = 8, activation = 'softmax') # 1000 neuron fully connected layer with ReLU activation\n",
        "\n",
        "# Connect input layer, base model and output layers\n",
        "inputs = Input(shape = (224, 224, 3)) # Input layer: 224x224 RGB images\n",
        "model = base_model(inputs)            # Feature extraction layers of InceptionV3\n",
        "model = global_average_pool(model)    # Global average pooling layer\n",
        "outputs = classifier(model)           # Fully connected layer: 8 neuron softmax activation for classification\n",
        "\n",
        "# Build inception_v3 model\n",
        "mobile_net_v2 = Model(inputs, outputs, name = 'MobileNetV2')\n",
        "\n",
        "# ---------------------------- Train MobileNetV2 -------------------------- #\n",
        "# RGB value normalization to improve learning rate - RGB values within activation function range\n",
        "def process(image, label):\n",
        "    image = cast(image/255.0, 'float32')\n",
        "    return image, label\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 4   # Batch size for gradient learning\n",
        "img_height = 224 # Image pixel height\n",
        "img_width = 224  # Image pixel width\n",
        "epochs = 50      # Arbitrarily large epoch for training\n",
        "\n",
        "# Define training and validation directory paths\n",
        "train_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset'\n",
        "validation_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset'\n",
        "\n",
        "# Create training dataset\n",
        "train_ds = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize training dataset pixel values to improve learning performance\n",
        "train_ds = train_ds.map(process)\n",
        "# Prefetch training dataset to improve computational speed\n",
        "train_ds = train_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Define validation dataset\n",
        "val_ds = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize validation pixel values to improve learning performance\n",
        "val_ds = val_ds.map(process)\n",
        "# Prefetch validation dataset to improve computation speed\n",
        "val_ds = val_ds.cache().prefetch(buffer_size = autotune)\n",
        "\n",
        "# Compile model:\n",
        "  # Adam algorithm optimization\n",
        "  # Categorical crossentropy loss function for multilabel classification\n",
        "  # Metrics: accuracy, precision, recall\n",
        "mobile_net_v2.compile(\n",
        "    optimizer = 'adam',\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics = ['accuracy']\n",
        ")\n",
        "\n",
        "# Define checkpoint to save model parameters to epoch with best performance\n",
        "model_checkpoint = ModelCheckpoint('/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Augmented Training/Checkpoint', monitor = 'val_accuracy', mode = 'max')\n",
        "\n",
        "# Train model to run until epochs finished or callback threshold reached\n",
        "mobile_net_v2.fit(\n",
        "    train_ds,\n",
        "    validation_data = val_ds,\n",
        "    epochs = epochs,\n",
        "    verbose = 1,\n",
        "    callbacks = [model_checkpoint]\n",
        ")\n",
        "\n",
        "# Save MobileNetV2 model at end of training\n",
        "save_model(model = mobile_net_v2, \n",
        "           filepath = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Models/MobileNetV2/Augmented Training/End',\n",
        "           include_optimizer = True,\n",
        ")\n",
        "\n",
        "# ---------------------------- Test MobileNetV2 -------------------------- #\n",
        "# Define testing dataset\n",
        "test_dir = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset'\n",
        "test_ds = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    validation_split = 0,\n",
        "    seed = 123,\n",
        "    image_size = (img_height, img_width),\n",
        "    batch_size = 1,\n",
        "    label_mode = 'categorical'\n",
        ")\n",
        "# Normalize testing pixel values\n",
        "val_ds = val_ds.map(process)\n",
        "\n",
        "# Perform predictions on testing dataset to obtain accuracy, precision and recall\n",
        "loss, accuracy = mobile_net_v2.evaluate(test_ds)\n",
        "loss, precision = mobile_net_v2.evaluate(test_ds)\n",
        "loss, recall = mobile_net_v2.evaluate(test_ds)\n",
        "print('\\nTest Accuracy: ' + str(accuracy))\n",
        "print('Test Precision: ' + str(precision))\n",
        "print('Test Recall: ' + str(recall))"
      ],
      "metadata": {
        "id": "Nwy6Ikt95Yso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation and Preprocessing"
      ],
      "metadata": {
        "id": "Z17a00QembaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Augmentor"
      ],
      "metadata": {
        "id": "d2COUrNmspTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41df8880-1b41-4946-d4b4-17512ba003ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Augmentor\n",
            "  Downloading Augmentor-0.2.10-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (4.64.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (7.1.2)\n",
            "Installing collected packages: Augmentor\n",
            "Successfully installed Augmentor-0.2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Dataset Split and Preprocessing: 70:20:10 (training, validation, testing) split*"
      ],
      "metadata": {
        "id": "G9yVAClFUxpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import directory navigation libraries and image resizing\n",
        "from os import listdir\n",
        "from shutil import move\n",
        "from random import randint\n",
        "from PIL import Image\n",
        "from tensorflow.image import resize\n",
        "from tensorflow.io import decode_jpeg, encode_jpeg, write_file\n",
        "from tensorflow.dtypes import saturate_cast\n",
        "\n",
        "# Constant values\n",
        "base_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/'\n",
        "base_validation_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset/'\n",
        "base_test_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset/'\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# ------------------------------- Functions -------------------------------- #\n",
        "# Resize images\n",
        "def resize_image(path, img_width, img_height):\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        image = open(path+'/'+file, 'rb') # Open image as binary file\n",
        "        binary_representation = image.read() # Read binary file\n",
        "        decoded_representation = decode_jpeg(binary_representation) # Decode image\n",
        "        resized_image = resize(decoded_representation, [img_width, img_height])\n",
        "        resized_image = saturate_cast(resized_image, 'uint8')\n",
        "        encoded_image = encode_jpeg(resized_image)\n",
        "        write_file(path+'/'+file, encoded_image)\n",
        "\n",
        "# Check images for RGB mode and convert if required\n",
        "def convert_rgb_images(path):\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        img = Image.open(path+'/'+file)\n",
        "        if img.mode != 'RGB':\n",
        "            img.convert('RGB').save(path+'/'+file)\n",
        "        \n",
        "# Count number of files in directory\n",
        "def count_files(inputPath, label):\n",
        "    directory = listdir(inputPath+label)\n",
        "    num_files = 0\n",
        "    for file in directory:\n",
        "        num_files += 1\n",
        "    return num_files\n",
        "\n",
        "# Split training and testing dataset\n",
        "def split_dataset(input_path, train_path, val_path, test_path, val_prop, test_prop, label):\n",
        "    # Count images in directory and get files names\n",
        "    num_files = count_files(input_path, label)\n",
        "    file_names = listdir(input_path+label)\n",
        "\n",
        "    # Calculated number of validation and testing images\n",
        "    num_val = round(num_files*val_prop)\n",
        "    num_test = round(num_files*test_prop)\n",
        "    \n",
        "    # Initialise empty lists for validation and testing image indexes\n",
        "    val_list = []\n",
        "    test_list = []\n",
        "    \n",
        "    # Randomly assign file indexes for validation dataset\n",
        "    i = 0\n",
        "    while i < num_val:\n",
        "        r = randint(1, num_files - 1)\n",
        "        if r not in val_list:\n",
        "            val_list.append(r)\n",
        "            i += 1\n",
        "    # Move validation dataset files and rename\n",
        "    name_incrementer = 1\n",
        "    for j in val_list:\n",
        "        move(input_path+label+'/'+file_names[j], val_path+label+'/'+label+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "\n",
        "    # Update number of files and directory for remaining files\n",
        "    num_files = count_files(input_path, label)\n",
        "    remaining_files = listdir(input_path+label)\n",
        "    # Randomly assign file indexes for testing dataset\n",
        "    i = 0\n",
        "    while i < num_test:\n",
        "        r = randint(1, num_files - 1)\n",
        "        if r not in test_list:\n",
        "            test_list.append(r)\n",
        "            i += 1\n",
        "    # Move test dataset files and rename\n",
        "    name_incrementer = 1\n",
        "    for j in test_list:\n",
        "        move(input_path+label+'/'+remaining_files[j], test_path+label+'/'+label+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "\n",
        "    # Move remaining files into training dataset\n",
        "    training_files = listdir(input_path+label)\n",
        "    name_incrementer = 1\n",
        "    for file in training_files:\n",
        "        move(input_path+label+'/'+file, train_path+label+'/'+label+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "\n",
        "# Print number of files in label directories\n",
        "def print_num_files(path, label):\n",
        "    directory = listdir(path+label)\n",
        "    counter = 0\n",
        "    for file in directory:\n",
        "        counter += 1\n",
        "    print(\"Number of files in \" + label +':' + str(counter))\n",
        "\n",
        "# ----------------- Split Training and Validation Datasets ----------------- #\n",
        "# Print split label contents\n",
        "print('# --------------- Total Label Counts ---------------- #')\n",
        "# Split labels base off 70:20:10 training split\n",
        "for label in labels:\n",
        "    print_num_files(base_path+'Unsplit Dataset/', label)\n",
        "    split_dataset(base_path+'Unsplit Dataset/', base_original_training_path, base_validation_path, base_test_path, 0.2, 0.1, label)\n",
        "\n",
        "# Print validation counts for each label\n",
        "print('\\n# ---------- Total Validation Label Counts ---------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_validation_path, label)\n",
        "\n",
        "# Print testing counts for each label\n",
        "print('\\n# ------------ Total Testing Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_test_path, label)\n",
        "\n",
        "# Print training counts for each label in training dataset\n",
        "print('\\n# ----------- Total Training Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)\n",
        "\n",
        "# ------------------------- Pre-Process Images -------------------------- #\n",
        "# Pre-process training dataset\n",
        "print('\\n# --------- Pre-Processing Training Dataset --------- #')\n",
        "for label in labels:\n",
        "    convert_rgb_images(base_original_training_path+label)\n",
        "    resize_image(base_original_training_path+label, 224, 224)\n",
        "# Pre-process validation dataset\n",
        "print('\\n# -------- Pre-Processing Validation Dataset -------- #')\n",
        "for label in labels:\n",
        "    convert_rgb_images(base_original_training_path+label)\n",
        "    resize_image(base_validation_path+label, 224, 224)\n",
        "# Pre-process test dataset\n",
        "print('\\n# --------- Pre-Processing Testing Dataset --------- #')\n",
        "for label in labels:\n",
        "    convert_rgb_images(base_original_training_path+label)\n",
        "    resize_image(base_test_path+label, 224, 224)"
      ],
      "metadata": {
        "id": "_jR-D3y89F57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augementation Set 1: Distort and Flip*"
      ],
      "metadata": {
        "id": "GgyR_bKCOuCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "from os import listdir\n",
        "from time import sleep\n",
        "\n",
        "# Define base path of training dataset\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "# Define labels\n",
        "#labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "labels = ['Glass']\n",
        "\n",
        "# Verify augmentation counts and print\n",
        "def count_augmented_and_print(path):\n",
        "    count_augmented = 0\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        count_augmented += 1\n",
        "    print('Processed with '+str(count_augmented)+' image(s) found.\\n')\n",
        "\n",
        "# Geometric augmentation function: distort and flip\n",
        "def distort_and_flip(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.flip_left_right(1)                   # Horizontally flip every image\n",
        "    pipe.random_distortion(1, 5, 5, 4)        # Randomly distort flipped images\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "    count_augmented_and_print(path+'/output') # Print augmented image count to console\n",
        "    sleep(60)                                 # Sleep program while images processing\n",
        "\n",
        "# Perform augmentation\n",
        "for label in labels:\n",
        "    distort_and_flip(base_original_training_path+label)"
      ],
      "metadata": {
        "id": "WR_iz_J4OtDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmentation Set 2: Rotate and Shear*"
      ],
      "metadata": {
        "id": "9l2NeImgQjJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "from os import listdir\n",
        "from time import sleep\n",
        "\n",
        "# Define base path of training dataset\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "# Define labels\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# Verify augmentation counts and print\n",
        "def count_augmented_and_print(path):\n",
        "    count_augmented = 0\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        count_augmented += 1\n",
        "    print('Processed with '+str(count_augmented)+' image(s) found.\\n')\n",
        "\n",
        "# Geometric augmentation: rotate and shear\n",
        "def rotate_and_shear(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.rotate(1, 25, 25)                    # Rotate every image by 45 degrees\n",
        "    pipe.shear(1, 15, 15)                     # Randomly shear between 0 and 15 degrees\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "    count_augmented_and_print(path+'/output') # Print augmented image count to console\n",
        "    sleep(60)                                 # Sleep program while images processing\n",
        "\n",
        "# Augment images\n",
        "for label in labels:\n",
        "    rotate_and_shear(base_original_training_path+label)"
      ],
      "metadata": {
        "id": "wj0yjMiiQjQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c10ea9a-8794-41e5-c9d8-450137ab0ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 351 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Glass/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A55F922D0>: 100%|| 351/351 [00:07<00:00, 44.23 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 351 image(s) found.\n",
            "\n",
            "Initialised with 338 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Plastic/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A55FC6E10>: 100%|| 338/338 [00:07<00:00, 44.30 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 338 image(s) found.\n",
            "\n",
            "Initialised with 416 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Paper/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A54E9C290>: 100%|| 416/416 [00:09<00:00, 43.83 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 416 image(s) found.\n",
            "\n",
            "Initialised with 287 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Metal/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A5572E310>: 100%|| 287/287 [00:06<00:00, 44.51 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 287 image(s) found.\n",
            "\n",
            "Initialised with 282 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Cardboard/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A5572E350>: 100%|| 282/282 [00:06<00:00, 44.46 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 282 image(s) found.\n",
            "\n",
            "Initialised with 363 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Vegetation/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A54E82750>: 100%|| 363/363 [00:10<00:00, 35.48 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 363 image(s) found.\n",
            "\n",
            "Initialised with 218 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Food/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A55707E50>: 100%|| 218/218 [00:05<00:00, 43.01 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 218 image(s) found.\n",
            "\n",
            "Initialised with 178 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Trash/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.Image.Image image mode=RGB size=224x224 at 0x7F7A55FC6190>: 100%|| 178/178 [00:04<00:00, 43.12 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 178 image(s) found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Augmentation Set 3: Random Erasing*"
      ],
      "metadata": {
        "id": "HiBgc25eQjhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data augmentation functions\n",
        "import Augmentor\n",
        "from os import listdir\n",
        "from time import sleep\n",
        "\n",
        "# Define base path of training dataset\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "# Define labels\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# Verify augmentation counts and print\n",
        "def count_augmented_and_print(path):\n",
        "    count_augmented = 0\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        count_augmented += 1\n",
        "    print('Processed with '+str(count_augmented)+' image(s) found.\\n')\n",
        "\n",
        "# Random occlusion: random erasing \n",
        "def random_erase(path):\n",
        "    pipe = Augmentor.Pipeline(path)           # Create pipeline in path of dataset\n",
        "    pipe.random_erasing(1, 0.15)              # Randomly erase area in every image\n",
        "    pipe.process()                            # Process images in pipeline\n",
        "    count_augmented_and_print(path+'/output') # Print augmented image count to console\n",
        "    sleep(60)                                 # Sleep program while images processing\n",
        "\n",
        "# Augment images\n",
        "for label in labels:\n",
        "    random_erase(base_original_training_path+label)\n",
        "\n",
        "# Second augmentation\n",
        "for label in labels:\n",
        "    random_erase(base_original_training_path+label+'/output')"
      ],
      "metadata": {
        "id": "AvgK26ktQjoE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a298a854-3969-45d9-98b3-6d904f1d1a54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialised with 351 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Glass/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55E87C50>: 100%|| 351/351 [00:04<00:00, 74.44 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 351 image(s) found.\n",
            "\n",
            "Initialised with 338 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Plastic/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55EF2F10>: 100%|| 338/338 [00:04<00:00, 74.01 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 338 image(s) found.\n",
            "\n",
            "Initialised with 416 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Paper/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55E2CD50>: 100%|| 416/416 [00:05<00:00, 75.13 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 416 image(s) found.\n",
            "\n",
            "Initialised with 287 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Metal/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55F12090>: 100%|| 287/287 [00:03<00:00, 74.63 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 287 image(s) found.\n",
            "\n",
            "Initialised with 282 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Cardboard/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55F52B10>: 100%|| 282/282 [00:03<00:00, 77.69 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 282 image(s) found.\n",
            "\n",
            "Initialised with 363 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Vegetation/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55EA0CD0>: 100%|| 363/363 [00:05<00:00, 67.59 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 363 image(s) found.\n",
            "\n",
            "Initialised with 218 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Food/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55F404D0>: 100%|| 218/218 [00:02<00:00, 76.68 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 218 image(s) found.\n",
            "\n",
            "Initialised with 178 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Trash/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55F3EF10>: 100%|| 178/178 [00:02<00:00, 70.25 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 178 image(s) found.\n",
            "\n",
            "Initialised with 351 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Glass/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55E61A50>: 100%|| 351/351 [00:04<00:00, 72.56 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 351 image(s) found.\n",
            "\n",
            "Initialised with 338 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Plastic/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55E52990>: 100%|| 338/338 [00:04<00:00, 74.27 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 338 image(s) found.\n",
            "\n",
            "Initialised with 416 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Paper/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55731F90>: 100%|| 416/416 [00:05<00:00, 71.12 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 416 image(s) found.\n",
            "\n",
            "Initialised with 287 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Metal/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A5DBD0390>: 100%|| 287/287 [00:03<00:00, 73.86 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 287 image(s) found.\n",
            "\n",
            "Initialised with 282 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Cardboard/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55731990>: 100%|| 282/282 [00:03<00:00, 75.48 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 282 image(s) found.\n",
            "\n",
            "Initialised with 363 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Vegetation/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A564E0510>: 100%|| 363/363 [00:04<00:00, 74.07 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 363 image(s) found.\n",
            "\n",
            "Initialised with 218 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Food/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55727710>: 100%|| 218/218 [00:02<00:00, 76.77 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 218 image(s) found.\n",
            "\n",
            "Initialised with 178 image(s) found.\n",
            "Output directory set to /content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/Trash/output/output."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224 at 0x7F7A55F3FA10>: 100%|| 178/178 [00:02<00:00, 74.39 Samples/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed with 178 image(s) found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Rename Files*"
      ],
      "metadata": {
        "id": "lVUDecD6ukHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from os import listdir\n",
        "from shutil import move\n",
        "\n",
        "# Constants\n",
        "set_one_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/1 Distort and Flip/'\n",
        "set_two_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/2 Rotate and Shear/'\n",
        "set_three_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/3 Random Erasing/'\n",
        "set_one_augmentation = 'distort_and_flip'\n",
        "set_two_augmentation = 'shear_and_rotate'\n",
        "set_three_augmentation = 'random_erase'\n",
        "labels = ['Glass', 'Plastic', 'Paper', 'Metal', 'Cardboard', 'Vegetation', 'Food', 'Trash']\n",
        "\n",
        "# Get files in directory and rename\n",
        "def rename_files(path, base_name, augmentation):\n",
        "    name_incrementer = 1\n",
        "    directory = listdir(path)\n",
        "    for file in directory:\n",
        "        move(path+'/'+file, path+'/'+base_name+'_'+augmentation+'_'+str(name_incrementer)+'.jpg')\n",
        "        name_incrementer += 1\n",
        "        print(file)\n",
        "    return name_incrementer\n",
        "\n",
        "# Rename files\n",
        "for label in labels:\n",
        "    rename_files(set_one_path+label+'/output', label, set_one_augmentation)\n",
        "    rename_files(set_two_path+label+'/output', label, set_two_augmentation)\n",
        "    rename_files(set_three_path+label+'/output', label, set_three_augmentation)"
      ],
      "metadata": {
        "id": "3M7P4vonum1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*File Counts*"
      ],
      "metadata": {
        "id": "yhoBeSzIB3HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import file navigation library\n",
        "from os import listdir\n",
        "\n",
        "# Directory paths\n",
        "base_validation_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Validation Dataset/'\n",
        "base_test_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Testing Dataset/'\n",
        "base_original_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Original Training Dataset/'\n",
        "base_augmented_training_path = '/content/drive/MyDrive/Colab Notebooks/Model Training and Evaluation/Augmented Training Dataset/'\n",
        "\n",
        "# Print number of files in label directories\n",
        "def print_num_files(path, label):\n",
        "    directory = listdir(path+label)\n",
        "    counter = 0\n",
        "    for file in directory:\n",
        "        counter += 1\n",
        "    print(\"Number of files in \" + label +':' + str(counter))\n",
        "\n",
        "# Print training counts for each label in training dataset\n",
        "print('\\n# ----------- Total Training Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)\n",
        "\n",
        "# Print validation counts for each label\n",
        "print('\\n# ---------- Total Validation Label Counts ---------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_validation_path, label)\n",
        "\n",
        "# Print testing counts for each label\n",
        "print('\\n# ------------ Total Testing Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_test_path, label)\n",
        "\n",
        "# Print training counts for each label in training dataset\n",
        "print('\\n# ----------- Total Training Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)\n",
        "\n",
        "# Print augmented counts for each label in augmented training dataset\n",
        "print('\\n# ----------- Total Augmented Label Counts ----------- #')\n",
        "for label in labels:\n",
        "    print_num_files(base_original_training_path, label)"
      ],
      "metadata": {
        "id": "4iqFvvDOB2dA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}